{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecurrentMemoryTransformer Training using Huggingface Trainer\n",
    "\n",
    "This notebook demonstrates how to train RecurrentMemoryTransformer model using Huggingface Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/openr1_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    AutoTokenizer, \n",
    "    AutoConfig, \n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import RecurrentMemoryTransformer modules\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from recurrent_memory_transformer.RecurrentMemoryTransformer import RecurrentMemoryTransformer\n",
    "from recurrent_memory_transformer.PreTrainedRMTConfig import PreTrainedRMTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path and model parameters setup\n",
    "dataset_path = \"HuggingFaceFW/fineweb-edu\"\n",
    "dataset_name = \"CC-MAIN-2024-10\"\n",
    "\n",
    "# Base model setup\n",
    "base_model_name = \"gpt2\"  # Can be changed to any base model\n",
    "\n",
    "# RMT parameters\n",
    "is_memory_all = True\n",
    "max_n_segments = 3\n",
    "input_seg_len = 512\n",
    "output_seg_len = 512\n",
    "align = \"left\"\n",
    "num_mem_tokens = 10\n",
    "\n",
    "# Training parameters\n",
    "output_dir = \"./rmt_model_output\"\n",
    "learning_rate = 5e-5\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "num_train_epochs = 0.1\n",
    "max_seq_length = 1024\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score'],\n",
      "        num_rows: 19883\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„É≠„Éº„Éâ\n",
    "dataset = load_dataset(dataset_path, dataset_name)\n",
    "\n",
    "dataset['train'] = dataset['train'].train_test_split(test_size=0.999, seed=42)['train']\n",
    "if  \"test\" not in dataset:\n",
    "    try:\n",
    "        dataset = dataset[\"train\"].train_test_split(test_size=100, seed=42)\n",
    "    except:\n",
    "        dataset = dataset.train_test_split(test_size=100, seed=42)\n",
    "            \n",
    "print(f\"Dataset loaded: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆ„É≠„Éº„Éâ\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂâçÂá¶ÁêÜÈñ¢Êï∞\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "\n",
    "# „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂâçÂá¶ÁêÜ\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[col for col in dataset[\"train\"].column_names if col != \"text\"],\n",
    "    desc=\"Tokenizing dataset\",\n",
    ")\n",
    "\n",
    "# „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÇíË®≠ÂÆö\n",
    "tokenized_dataset = tokenized_dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RecurrentMemoryTransformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with config: PreTrainedRMTConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"align\": \"left\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"base_model_config\": {\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"_name_or_path\": \"gpt2\",\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"sep_token_id\": null,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": {\n",
      "      \"text-generation\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_length\": 50\n",
      "      }\n",
      "    },\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "  },\n",
      "  \"base_model_type\": \"gpt2\",\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_seg_len\": 512,\n",
      "  \"is_memory_all\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_n_segments\": 3,\n",
      "  \"model_type\": \"rmt\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_mem_tokens\": 10,\n",
      "  \"output_seg_len\": 512,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# „Éô„Éº„Çπ„É¢„Éá„É´„ÅÆË®≠ÂÆö„Çí„É≠„Éº„Éâ\n",
    "base_config = AutoConfig.from_pretrained(base_model_name)\n",
    "\n",
    "# RecurrentMemoryTransformer„ÅÆË®≠ÂÆö„Çí‰ΩúÊàê\n",
    "rmt_config = PreTrainedRMTConfig(\n",
    "    base_model_config=base_config,\n",
    "    base_model_type=base_model_name,\n",
    "    is_memory_all=is_memory_all,\n",
    "    max_n_segments=max_n_segments,\n",
    "    input_seg_len=input_seg_len,\n",
    "    output_seg_len=output_seg_len,\n",
    "    align=align,\n",
    "    num_mem_tokens=num_mem_tokens\n",
    ")\n",
    "\n",
    "# „Éô„Éº„Çπ„É¢„Éá„É´„Çí„É≠„Éº„Éâ\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "# RecurrentMemoryTransformer„É¢„Éá„É´„ÇíÂàùÊúüÂåñ\n",
    "model = RecurrentMemoryTransformer(rmt_config, base_model=base_model)\n",
    "print(f\"Model initialized with config: {rmt_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collator Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„Çø„Ç≥„É¨„Éº„Çø„ÉºÔºàË®ÄË™û„É¢„Éá„É™„É≥„Ç∞Áî®Ôºâ„ÅÆË®≠ÂÆö\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # MLM„Åß„ÅØ„Å™„ÅèCLM„Çí‰ΩøÁî®\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Setup and Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/openr1_venv/lib/python3.11/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_805429/3375379673.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-17 19:25:11,169] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# „Éà„É¨„Éº„Éã„É≥„Ç∞ÂºïÊï∞„ÅÆË®≠ÂÆö\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    fp16=torch.cuda.is_available(),  # GPU„ÅåÂà©Áî®ÂèØËÉΩ„Å™„ÇâÂçäÁ≤æÂ∫¶„ÅßÂ≠¶Áøí\n",
    "    gradient_accumulation_steps=2,   # ÂãæÈÖçËìÑÁ©ç„Çπ„ÉÜ„ÉÉ„Éó\n",
    ")\n",
    "\n",
    "# Trainer„ÅÆÂàùÊúüÂåñ\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"] if \"test\" in tokenized_dataset else None,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshin2021001\u001b[0m (\u001b[33mshin2021001-osaka-city-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/app/Recurrent-Memory-Transformer_PreTrained/recurrent_memory_transformer/examples/wandb/run-20250317_192513-7sp1tocz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shin2021001-osaka-city-university/huggingface/runs/7sp1tocz' target=\"_blank\">./rmt_model_output</a></strong> to <a href='https://wandb.ai/shin2021001-osaka-city-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shin2021001-osaka-city-university/huggingface' target=\"_blank\">https://wandb.ai/shin2021001-osaka-city-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shin2021001-osaka-city-university/huggingface/runs/7sp1tocz' target=\"_blank\">https://wandb.ai/shin2021001-osaka-city-university/huggingface/runs/7sp1tocz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/openr1_venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 02:57, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics: {'train_runtime': 181.4286, 'train_samples_per_second': 10.959, 'train_steps_per_second': 0.689, 'total_flos': 1045262499840000.0, 'train_loss': 3.598515930175781, 'epoch': 0.1005631536604988}\n",
      "Model saved to ./rmt_model_output\n"
     ]
    }
   ],
   "source": [
    "# „É¢„Éá„É´„ÅÆÂ≠¶Áøí„ÇíÂÆüË°å\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Â≠¶ÁøíÁµêÊûú„Å®ÊåáÊ®ô„ÅÆË°®Á§∫\n",
    "print(f\"Training metrics: {train_result.metrics}\")\n",
    "\n",
    "# „É¢„Éá„É´„ÅÆ‰øùÂ≠ò\n",
    "trainer.save_model()\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „É¢„Éá„É´„ÅÆË©ï‰æ°ÔºàÊ§úË®º„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºâ\n",
    "if \"validation\" in tokenized_dataset:\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is a day of celebration for all of us. It is a day of celebration for all of us. It is a day of celebration for all of us. It is a day of celebration for all of us.\n"
     ]
    }
   ],
   "source": [
    "# Test text generation\n",
    "test_input = \"Today is a wonderful day.\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids = inputs[\"input_ids\"], max_length=50)\n",
    "out_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(out_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
