{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/openr1_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from recurrent_memory_transformer import RecurrentMemoryTransformer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'KotShinZ/gpt2-RMT-8'\n",
    "max_length = 1004 * 2 # 1024 - memory size * 2\n",
    "dataset_path = \"HuggingFaceFW/fineweb-edu\"\n",
    "dataset_name = \"CC-MAIN-2024-10\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_path, name=dataset_name, split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, tokenizer, dataset, max_length, stride = None):\n",
    "    \"\"\"Model evaluation\n",
    "    Args:\n",
    "        model: Model\n",
    "        tokenizer: Tokenizer\n",
    "        dataset: Dataset\n",
    "        max_length: Maximum token count\n",
    "        stride: Stride\n",
    "    Returns:\n",
    "        loss: Loss\n",
    "        ppl: Perplexity\n",
    "    \n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = max_length / 2\n",
    "\n",
    "    # トークン化\n",
    "    encodings = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")\n",
    "    \n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    loss = torch.stack(nlls).mean()\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())  \n",
    "    return loss, ppl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type rmt_gpt2 to instantiate a model of type rmt. This is not supported for all configurations of models and can yield errors.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1087994 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(2.9417, device='cuda:0'), tensor(18.9489, device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RecurrentMemoryTransformer.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "eval_model(model, tokenizer, dataset, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am a student at the University of California, Berkeley. I am a member of the American Association of University Professors, and I am a member of the American Association of University Professors. I am also a member of\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids = inputs[\"input_ids\"], max_length=50)\n",
    "out_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(out_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
